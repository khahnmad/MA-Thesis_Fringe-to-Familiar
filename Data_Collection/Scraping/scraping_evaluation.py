import glob
import universal_functions as uf
from collections import Counter
import random
import time
from typing import List

from Data_Collection.Scraping import scrape_urls as scrape
from Data_Collection.Scraping import error_handling as e

scrape_dir = uf.repo_loc / 'Data_Collection/Scraping'
mediacloud_dir = uf.repo_loc / 'Data_Collection/Media_Cloud'

# GLOBAL
all_scraping_evals = [x for x in glob.glob(str(scrape_dir) + "/*.csv")]
manual_eval_dict = uf.load_manual_eval_as_dict()


# FUNCTIONS
def get_uncaught_errors(dataset: list, i: int):
    # Check text and url for uncaught errors
    error = None
    text = dataset[i][-1]
    url = dataset[i][3]
    # Check url in case the website is no longer maintained

    for obj in e.n_scrapable_url_errors['Website is no longer maintained']:
        if obj in url:
            text = 'ERROR: Website is no longer maintained'
            error = [i, dataset[i][9], text]

    # Check text for language that indicates the text scraped is actually an error message
    for elt in e.empty_ptag_text_errors['Blocked from website']:
        if elt in text.lower():
            text = 'ERROR: Blocked from website'
            error = [i, dataset[i][9], text]

    for item in e.soup_contents_text_errors['Article not found']:
        if item in text.lower():
            text = 'ERROR: Article not found'
            error = [i, dataset[i][9], text]

    for p in e.soup_contents_text_errors['Article behind a paywall or login page']:
        if p in text.lower():
            text = "ERROR: Article behind a paywall or login page"
            error = [i, dataset[i][9], text]

    for l in e.soup_contents_text_errors['451 Unavailable for legal reasons']:
        if l in text.lower():
            text = 'ERROR: 451 Unavailable for legal reasons'
            error = [i, dataset[i][9], text]

    if 'reported a bad gateway error' in dataset[i][-1].lower():
        text = 'ERROR: 502 gateway error'
        error = [i, dataset[i][9], text]
    if 'This error was generated by Mod_Security'.lower() in dataset[i][-1].lower():
        text = 'ERROR: Mod_Security server-side error'
        error = [i, dataset[i][9], text]
    if 'Varnish cache server' in dataset[i][-1]:
        text = 'ERROR: Not able to scrape - cache issue'
        error = [i, dataset[i][9], text]
    if 'msnbc' in dataset[i][3] and 'watch' in dataset[i][3]:
        text = 'ERROR: Video/ Image content only'
        error = [i, dataset[i][9], text]
    if 'adelaidenow.com' in dataset[i][3] and 'video' in dataset[i][3]:
        text = 'ERROR: Video/ Image content only'
        error = [i, dataset[i][9], text]
    if 'us-job-market-strong-start' in dataset[i][3]:
        text = 'ERROR: Video/ Image content only'
        error = [i, dataset[i][9], text]
    if 'client-side exception' in dataset[i][-1].lower():
        text = 'ERROR: Client-Side Exception'
        error = [i, dataset[i][9], text]
    if 'please enable js' in text.lower():
        text =  'ERROR: Must enable cookies to access site'
        error = [i, dataset[i][9], text]
    return text, error  # If there's no error : text= original text, error = None


def print_most_common(items: list):
    counter = Counter(items)
    most = counter.most_common()
    for thing in most:
        print(f"    {thing}")
    return most


def generate_export_name(filename: str) -> str:
    # Creates unique export names for the first two times it runs, then just updates the version 2 for runs after that
    midpoint_export_name = filename.replace('datasets', 'scraping_evals')
    midpoint_export_name = midpoint_export_name.replace('text', 'eval')

    if midpoint_export_name in all_scraping_evals:
        midpoint_export_name = midpoint_export_name[:-4] + '_1.csv'
        if midpoint_export_name in all_scraping_evals:
            midpoint_export_name = midpoint_export_name[:-5] + '2.csv'
    return midpoint_export_name


def retry_scrape(dataset: list, i: int):
    # Try to fix a bad scrape by scraping again

    # Initialize empty variables
    fixed = 0
    error = None

    url = dataset[i][3]  # Identify url
    text = scrape.access_article(url)  # Scrape the article again

    if 'ERROR:' in text:  # Check if the rescrape has an error
        error = [i, dataset[i][9], text]

    else:
        if text == '':  # If no text is gathered
            text = 'ERROR: No text gathered' # Often happens when I have some incomplete alternative scraping
            error = [i, dataset[i][9], text] # Debug: inspect the empty text - should have been handled earlier
        else:
            fixed = 1  # If text is gathered, text = text, error = None, fixed = 1
    return text, error, fixed


def investigate_error_messages(data: list, i: int):
    """
    Given an article with an error message in it (data[i]), see if we can fix the error message
    """
    handleable_errors = ['No text gathered', 'requests.exceptions.ReadTimeout', 'Server side connection error',
                         'requests.exceptions.ConnectionError',
                         'Client-Side Exception', 'Article not found', 'Scraping error during JSON conversion',
                         'could not be scraped', '404', '403']
    for elt in handleable_errors:
        if elt in data[i][-1]:
            text, error, success = retry_scrape(data, i)  # Try to scrape the article again
            return text, error, success

    if 'Blocked from website' in data[i][-1]:  # In this case also add an extra time wait
        random_wait = random.randint(1, 3)
        time.sleep(random_wait)
        text, error, success = retry_scrape(data, i)  # Try to scrape again
        return text, error, success

    return False, False, False


def check_for_prev_evaluations(filename):

    prev_evaluations = [x for x in glob.glob(str(scrape_dir) + "/*.csv")
                        if filename[48:-6].replace('text','eval') in x]
    if len(filename)==68 or len(filename)==70:
        prev_evaluations = [x for x in
                            glob.glob(str(scrape_dir) + "/*.csv")
                            if filename[48:-4].replace('text', 'eval') in x]
    rates = []
    fixed_num = 'no'
    for file in prev_evaluations:
        eval_file = uf.import_csv(file)
        fixed_num = eval_file[4][1]
        rates.append(float(eval_file[3][1])) # DEBUG: Check that this indexing is correct & that the eval files are in the right order
    improvement = []
    for i in range(len(rates)):
        if i > 0:
            improvement.append(rates[i]-rates[i-1])
            # print('check')

    print(f"{len(prev_evaluations)} previous evaluations exist for this file...")
    if len(prev_evaluations)>0:
        print(f"fixed {fixed_num} last time")
    if len(prev_evaluations)>1:
        print(f"with a {improvement[-1]} point improvement between the last two files")


def check_for_failed_scrapes(eval_result:List[List[int]]):
    if len(eval_result[1])>0:
        print(f"{len(eval_result[1])} scrape(s) marked incorrectly as successes")
    if len(eval_result[3])>0:
        print(f"{len(eval_result[3])} scrape(s) marked incorrectly as errors")


### ACTION ###
all_complete_text = [x for x in glob.glob(str(mediacloud_dir) + "/*.csv") if
                     'text' in x]
for item in all_complete_text:
    print(f"\nFor dataset: {item}")
    check_for_prev_evaluations(item)
    try:
        manual_eval = manual_eval_dict[item]  # Import the manual evaluation data from the file
    except KeyError:
        print(f'Missing manual evaluation for {item}')
        continue
    check_for_failed_scrapes(manual_eval)
    proceed = input("Enter 'y' to continue evaluating this file:")
    if proceed == 'y':
        # Initialize empty variables for the run
        errors, themes = [], []
        fixed = 0
        # Imports
        dataset = uf.import_csv(item)  # Import the file


        for i in range(1, len(dataset)):  # Iterate through the dataset
            if str(i).endswith("00"):
                print(f"Running {i} .... {(i/len(dataset))*100}% done")
            # Check that the row is the right length
            if len(dataset[i]) != 11:
                if len(dataset[i]) == 10:
                    given_row = dataset[i]
                    if 'ERROR:' in dataset[i][-1]:
                        dataset[i].append(dataset[i][-1])
                    else:
                        text, error, success = retry_scrape(dataset, i)
                        dataset[i].append(text)
                        errors.append(error)
                        fixed += success
                else:  # if len(dataset) != 11 and != 10
                    given_row = dataset[i] # for some reason this is triggered by scmp articles
                    dataset[i] = dataset[i][:11] # this assumes there are 12 elts
                    text, error, success = retry_scrape(dataset, i)
                    dataset[i][-1]=text
                    errors.append(error)
                    fixed += success
                    # print('    investigate weird length')

            # Check that the article is in English
            if dataset[i][4] != 'en':
                dataset[i][-1] = 'ERROR: Not in English'
                errors.append([i, dataset[i][9], dataset[i][-1]])
                continue

            # Check if the article is too short to be valid
            if len(dataset[i][-1]) < 370 and "ERROR:" not in dataset[i][-1]:
                given_row = dataset[i]
                # dataset[i][-1] = 'ERROR: Insufficient text'
                text, error, success= retry_scrape(dataset,i)
                if text == given_row[-1]:
                    text = 'ERROR: Insufficient text'
                dataset[i][-1] = text
                errors.append(error)
                fixed += success
                continue

            # Check if article is manually marked as incorrectly scraped
            if i in manual_eval[1]:
                given_row = dataset[i]  # Identify the row
                # text: scraped, # error: [i, text, error message] # success: 1/0
                text, error, success = retry_scrape(dataset, i)  # try scraping the article again
                dataset[i][-1] = text  # Replace previous text with newly scraped text
                errors.append(error)
                fixed += success

            # Check if article is manually marked as incorrectly error messaged
            if i in manual_eval[3]:
                given_row = dataset[i]  # Identify the row
                text, error, success = retry_scrape(dataset, i)
                dataset[i][-1] = text
                errors.append(error)
                fixed += success

            # Check if the url is in this list of urls that may have been messed up in the past
            double_check_urls = ['scmp.com', 'nationalreview.com', 'qz.com', 'columbiaspectator.com', 'toledoblade',
                                 'newtimes.com.rw','nationalnewswatch.com','post-gazette','sgtreport.com'
                                 'blacknews.com/news', 'thepoliticalinsider', 'newsweek.com', 'sbs.com.au',
                                 'kyivpost.com','timesofindia','kake','refinery29',
                                 'conservativereview', 'washpo','thenewsdoctors.com']
            for obj in double_check_urls:
                if obj in dataset[i][3]:
                    given_row = dataset[i]
                    text, error, success = retry_scrape(dataset, i)
                    dataset[i][-1] = text
                    errors.append(error)
                    fixed += success

            if 'Die hotsplots GmbH, Rothers' in dataset[i][-1]:
                given_row = dataset[i]
                text, error, success = retry_scrape(dataset, i)
                dataset[i][-1] = text
                errors.append(error)
                fixed += success

            # If the article has an error message, check if it's an error that can be fixed
            if 'ERROR:' in dataset[i][-1]:
                # text, error, success = investigate_error_messages(dataset, i)
                text, error, success = investigate_error_messages(dataset, i)
                if any([text, error,
                        success]):  # If any of these is not False, update the dataset, errors, and success rate
                    dataset[i][-1] = text
                    errors.append(error)
                    fixed += success
                else:  # Indicates the error message was not handleable, so add the error message to the list
                    errors.append([i, dataset[i][9], dataset[i][-1]])

            if dataset[i][-1] == '' or dataset[i][-1] == 'nan':  # If nothing appears, try to scrape again
                given_row=dataset[i]
                text, error, success = retry_scrape(dataset, i)
                dataset[i][-1] = text
                errors.append(error)
                fixed += success

            # Look at the text and see if there is an uncaught error message in the text
            handled_text, error = get_uncaught_errors(dataset, i)
            dataset[i][-1] = handled_text
            errors.append(error)

            if dataset[i][6] != '':  # Check if themes are present
                themes.append([i, dataset[i][6]])
            # Done iterating through the dataset

        # Process the errors, success rate, exports

        # Check for duplicates in errors
        clean_errors = []
        for elt in errors:
            if elt not in clean_errors and elt is not None:
                clean_errors.append(elt)

        # Identify most common errors
        msgs_alone = [x[-1] for x in clean_errors]  # look at just the error messages
        most = print_most_common(msgs_alone)

        if len(dataset) > 1:  # As long as there is more than one item in the dataset
            # Calculate theme rate, success rate, and # fixed
            dataset_size = len(dataset) - 1
            theme_rate = (len(themes) / (dataset_size)) * 100
            successes = dataset_size - len(clean_errors)
            success_rate = (successes / dataset_size) * 100
            if success_rate < 0:
                print('Investigate')

            print(f"    Theme Rate: {theme_rate}%")
            print(f"    Success Url Rate: {success_rate}%")
            print(f"    Fixed: {fixed}\n")

            # Export the dataset to the same file name, updating the contents of the file
            uf.export_nested_list(item, dataset)

            # Create and export the information on results of the evaluation
            midpoint_export_name = generate_export_name(item)
            midpoint_dataset_export = [['Dataset name:', item],
                                       ['Most Common Errors:', most],
                                       ['Theme Rate:', theme_rate],
                                       ['Success Url Rate:', success_rate],
                                       ['Number fixed:', fixed]]
            uf.export_nested_list(midpoint_export_name, midpoint_dataset_export)
print('check')
